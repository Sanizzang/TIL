### 머신러닝?

컴퓨터가 데이터를 이용해 학습하는 알고리즘 기술 인공신경망, 결정 트리, 서포트 벡터 머신 등 다양한 방법

### 머신러닝 핵심용어 정리

- 머신러닝은 지도 학습, 비지도 학습, 강화 학습으로 나눌 수 있다.
- 머신러닝 프로세스는 데이터 수집 -> 데이터 전처리 -> 모델 학습 -> 모델 평가 -> 모델 활용(배포/시각화 등)으로 이루어 진다.
- 피처 엔지니어링은 모델 학습에 입력할 데이터를 더 풍성하고 가치 있게 만드는 작업이다.
  - 예를 들어 더미 변수를 생성하는 작업이 여기에 속한다.
- 독립변수는 예측에 사용되는 재료와 같은 변수들이다. 피처 변수(혹은 그냥 피처)라고도 부른다.
- 종속변수는 예측을 하려는 대상 변수이며, 목표 변수라고도 한다. 타깃이라고도 부른다.

### 머신러닝 개념 요약 정리

- 분류와 회귀
  - 회귀 평가지표
- 분류 평가지표
  - 오차 행렬
  - 로그 손실
  - ROC, AUC
- 데이터 인코딩
  - 레이블 인코딩
  - 원-핫 인코딩
- 피처 스케일링
  - min-max 정규화
  - 표준화
- 교차 검증
  - K 폴드
  - 층화 K 폴드
- 주요 머신러닝 모델
  - 선형 회귀
  - 로지스틱 회귀
  - 결정 트리
  - 앙상블
  - 랜덤 포레스트
  - XGBoost
  - LightGBM
- 하이퍼파라미터 최적화
  - 그리드서치
  - 랜덤서치
  - 베이지안 최적화

### 인공지능, 머신러닝, 딥러닝

- 인공지능
  : 인간의 뇌를 모방하는 모든 프로그램

- 머신러닝
  : 데이터를 기반으로 학습해 무언가를 예측하거나, 데이터 자체의 어떤 특성을 찾아내는 프로그램

- 딥러닝
  : 머신러닝 알고리즘 중 하나로, 인공 신경망에서 진화한 형태. 주로 이미지, 비디오, 자연어 분석에 쓰임

### 머신러닝 기법: 지도 학습, 비지도 학습, 강화 학습

- 회귀: 연속된 값을 측정하는 문제를 회귀라고 한다. 중고차 가격 예측과 같은 것이 해당
- 분류: 범주형 값을 측정하는 문제를 분류형 문제라고한다. 예를 들어 마케팅 문자를 보냈을 때 구매할 것 같은
  고객을 예측하는 문제는 Yes/No 두 가지 범주로 표현되므로 분류형 문제에 속한다.

- 군집화(clustering): 비슷한 데이터끼리 묶어주는 비지도 학습 방법이다. 소비 패턴이 유사한 카드 고객을 묶어주는 것과 같은
  용도로 사용할 수 있다.
- 변환(transformation): 목적에 따라 데이터를 다른 형태로 변환하는 방법이다. 예를 들어 변수 100개의 특성을 최대한 살려
  변수 2개로 압축하는 용도로 쓸 수 있다.
- 연관(association): 일명 장바구니 분석이라고도 하며, 유사한 구매 목록을 가진 고객끼리 비교하여, 서로의 장바구니에
  담기지 않은 새로운 아이템을 추천을 하는 목적등으로 사용할 수 있다.

### 머신러닝 핵심 라이브러리

- 넘파이: 다차원 배열에 대한 빠른 처리를 지원하는 라이브러리
- 판다스: 넘파이 기반으로 구현한 라이브러리. 인간 친화적인 자료구조를 제공해 데이터를 넘파이보다 쉽게 읽고 변형할 수 있음
- 사이킷런: 데이터 분석과 머신러닝 관련 다양한 기능을 지원하는 강력한 라이브러리. 데이터 전처리, 모델링, 모델 평가 등 굉장히 광범위한 분야 지원
- 맷플롯립: 시각화 라이브러리
- 시본: 맷플롯립 기반으로 구현한 라이브러리. 맷플롯립보다 사용이 쉽고 더 깔끔한 결과물을 보여줌
- 사이파이: 수학, 통계, 신호 처리, 이미지 처리, 함수 최적화에 사용되는 강력한 데이터 과학 라이브러리
- 테아노: 수학 표현식, 특히 행렬 값을 조작하고 평가하는 라이브러리

### 데이터 시각화 그래프 종류

- 선형 그래프
  : 연속된 변수에 대한 특정 값의 변화를 나타내는 데 적합한 그래프, 보통 x축에 시간처럼 연속된 변수를 두고 알고자 하는
  변수를 y축에 표현함
- 산점도
  : 두 변수 간의 전체적인 관계 및 그에 대한 분포를 표현하는 그래프. 각 데이터를 하나의 점으로 표현함
- 히스토그램
  : 도수분포표를 나타내는 그래프. x축의 값은 실제로 연속된 값이나, 이를 일정 간격으로 나누어 각 구간에 대한 도수를 y축으로 표현
- 박스 플롯
  : 데이터의 대략적인 분포(25%, 50%, 75% 지점)와 이상치 등을 간결하게 보여주는 그래프
- 히트맵
  : 데이터를 색상으로 표현하는 그래프. 다른 그래프들과 달리 여러 변수를 동시에 반영할 수 있음. 예를 들어 여러 변수 간의
  상관관계를 그래프로 표현하는데 유용함
- 막대 그래프
  : 히스토그램과 달리 특정 구간이 아닌 특정 범주에 대한 그래프를 그리는 그래프. 예를 들어 성별에 따른 마케팅 방응률을 표현할 때 사용 가능

### 분류와 회귀

- 분류
  - 분류(classification): 어떤 대상을 정해진 범주에 구분해 넣는 작업
  - 이진분류(binary classification): 타깃값이 두 개인 분류
  - 다중분류(multiclass classification): 타깃값이 세 개 이상인 분류
- 회귀

  - 독립변수(independent variable): 영향을 미치는 변수
    예) 학습 시간, 수면의 질, 공장의 재고 등
  - 종속변수(dependent variable): 영향을 받는 변수
    예) 시험 성적, 건강, 회사 이익 등
  - 회귀(regression): 독립변수와 종속변수 간 관계를 모델링하는 방법
  - 단순선형회귀(simple linear regression): 독립변수 하나(x)와 종속변수 하나(Y) 사이의 관계를 나타낸 모델링 기법
  - 다중선형회귀(multiple linear regression): 독립변수 여러 개와 종속변수 하나 사이의 관계를 나타낸 모델링 기법
  - 회귀 문제에서는 주어진 독립변수(피처)와 종속변수(타깃값) 사이의 관계를 기반으로 최적의 회귀계수를 찾아야 한다.

  - 상관계수(correlation coefficient): 두 변수 사이의 상관관계 정도를 수치로 나타낸 값
  - 피어슨 상관계수(pearson correlation coefficient): 선형 상관관계의 강도(strength)와 방향(direction)을 나타내며, -1부터
    1사이의 값을 갖는다. 상관계수가 음수면 음의 상관관계, 양수면 양의 상관관게가 있다 한다.

### 분류 평가지표

- 오차행렬(confusion matrix): 실제 타깃값과 예측 타깃값이 어떻게 매칭되는지를 보여주는 표

  - 정확도(accuracy): 실젯값과 예측값이 얼마나 일치되는지를 비율로 나타낸 평가지표
  - 정밀도(precision): 양성 예측의 정확도
  - 재현율(recall): 실제 양성 값(TP + FN)중 양성으로 잘 예측한 값(TP)의 비율.
    재현율은 민감도(sensitivity) 또는 참 양성 비율(true positive rate, TPR)라고도 한다.
  - F1 점수(F1 score): 정밀도와 재현율을 조합한 평가지표

- 로그손실(logloss): 분류 문제에서 타깃값을 확률로 예측할 때 기본적으로 사용하는 평가지표. 값이 작을수록 좋은 지표다.

- ROC(Receiver Operating Characteristic): 참 양성 비율(TPR)에 대한 거짓 양성 비율(False Positive Rate, FPR)곡선
- AUC(Area Under the Curve): ROC 곡선 아래 면적

### 데이터 인코딩

- 레이블 인코딩(label encoding)
  - 범주형 데이터를 숫자로 일대일 매핑해주는 인코딩 방식. 범주형 데이터를 숫자로 치환하는 것
- 원-핫 인코딩(one-hot encoding)
  - 여러 값 중 하나(one)만 활성화(hot)하는 인코딩

### 피처 스케일링

: 서로 다른 피처 값의 범위(최댓값 - 최솟값)가 일치하도록 조정하는 작업. 값의 범위가 데이터마다 다르면
모델 훈련이 제대로 안 될 수도 있다.

- min-max 정규화

  - 피처 값의 범위를 0~1로 조정하는 기법. 조정 후 최솟값은 0, 최댓값은 1이 된다.

- 표준화(standardization)
  - 평균이 0, 분산이 1이 되도록 피처 값을 조정하는 기법. min-max 정규화와 다르게 표준화는 상한과 하한이 없다.

### 교차 검증

#### K 폴드 교차 검증

1. 전체 훈련 데이터를 K개 그룹으로 나눈다.
2. 그룹 하나는 검증 데이터로, 나머지 K-1개는 훈련 데이터로 지정한다.
3. 훈련 데이터로 모델을 훈련하고, 검증 데이터로 평가한다.
4. 평가점수를 기록한다.
5. 검증 데이터를 다른 그룹으로 바꿔가며 2~4 절차를 K번 반복한다.
6. K개 검증 평가점수의 평균을 구한다

### 주요 머신러닝 모델

#### 선형 회귀 모델

- 선형 회귀(Linear Regression): 선형 회귀식을 활용한 모델, 선형 회귀 모델을 훈련한다는 것은 훈련 데이터에 잘 맞는 모델 파라미터,
  즉 회귀계수를 찾는 것이다.

#### 로지스틱 회귀 모델

- 로지스틱 회귀(Logistic Regression): 선형 회귀 방식을 응용해 분류에 적용한 모델. 스팸 메일일 확률을 구하는 이진 분류 문제에
  로지스틱 회귀를 사용할 수 있다.

#### 결정 트리

- 결정 트리(decision tree): 분류와 회귀 문제에 모두 사용 가능한 모델. '의사결정 나무'라고도 한다.
  - 불순도(impurity): 한 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지 나타내는 정도
  - 엔트로피(entropy): '불확실한 정도'
  - 정보 이득(information gain): 1에서 엔트로피를 뺀 수치 (1-엔트로피)
  - 지니 불순도(gini impurity): 엔트로피와 비슷한 개념. 지니 불순도 값이 클수록 불순도가 높고 작을수록
    불순도가 낮다.

#### 앙상블 학습(ensemble learning)

- 다양한 모델이 내린 예측 결과를 결합하는 기법. 앙상블 학습을 활용하면 대체로 예측 성능이 좋아진다. 과대적합 방지 효과도 있음
- 보팅(voting): 서로 다른 예측 결과가 여러 개 있을 때 개별 결과를 종합해 최종 결과를 결정하는 방식
- 하드 보팅(hard voting): '다수결 투표' 방식으로 최종 예측값을 정하는 방식
- 소프트 보팅(soft voting): 개별 예측 확률들의 평균을 최종 예측확률로 정하는 방식
- 배깅(bagging): 개별 모델로 예측한 결과를 결합해 최종 예측을 정하는 기법. '개별 모델이 서로 다른 샘플링 데이터를 활용'한다는 점이 특징이다.
- 부스팅(boosting): 가중치를 활용해 분류 성능이 약한 모델을 강하게 만드는 기법

#### 랜덤 포레스트(random forest)

: 결정 트리를 배깅 방식으로 결합한 모델. 나무(tree)가 모여 숲(forest)을 이루듯 결정 트리가 모여 랜덤 포레스트를 구성한다.
결정 트리와 마찬가지로 랜덤 포레스트도 분류와 회귀 문제에 모두 적용할 수 있다.

#### XGBoost

: 성능이 우수한 트리 기반 부스팅 알고리즘. 랜덤 포레스트는 결정 트리를 병렬로 배치하지만, XGBoost는 직렬로 배치해 사용한다.
즉 랜덤 포레스트는 배깅 방식, XGBoost는 부스팅 방식이다. XGBoost는 부스팅 방식이기 때문에 직전 트리가 예측한 값을 다음 트리가
활용해서 예측값을 조금씩 수정할 수 있다.

#### LightGBM

- XGBoost와 성능은 비슷하지만 훈련 속도가 더 빨라서 많은 캐글러가 가장 애용하는 머신러닝 모델. 마이크로소프트에서 개발했다.
- 말단 노드 중심으로 예측 오류를 최소화하게끔 분할한다. 말단 노드 중심으로 분할하면 균형을 유지할 필요가 없으니 추가 연산이 필요 없다.
  균형 중심 분할보다 빠르다. 하지만 데이터 개수가 적을 때는 과대적합되기 쉽다는 단점이 있다. (과대 적합 방지용 하이퍼파라미터를 조정해야 함)

### 하이퍼파라미터 최적화

: 하이퍼파라미터는 사용자가 직접 설정해야 하는 값이다. 모델이 좋은 성능을 내려면 어떤 하이퍼파라미터가 어떤 값을 가지면 좋을지를 찾아야 하며,
이를 하이퍼파라미터 최적화라고 한다.

- 그리드서치(grid search)
  : 가장 기본적인 하이퍼파라미터 최적화 기법. 주어진 하이퍼파라미터를 모두 순회하며 가장 좋은 성능을 내는 값을 찾는다.
  모든 경우의 수를 탐색하기 떄문에 시간이 오래 걸린다.
- 랜덤서치(random search)
  : 하이퍼파라미터를 무작위로 탐색해 가장 좋은 성능을 내는 값을 찾는 기법. 무작위라는 한계 때문에 그리드서치나 베이지안
  최적화에 비해 사용 빈도가 떨어진다.
- 베이지안 최적화(bayesian optimization)
  : 사전 정보를 바탕으로 최적 하이퍼파라미터 값을 확률적으로 추정하며 탐색하는 기법. 그리드서치나 랜덤서치보다 최적 하이퍼파라미터를
  더 빠르고 효율적으로 찾아준다. 코드도 직관적이며 사용하기 편리하다.
